{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1192a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib as jb\n",
    "\n",
    "#importing the dataset\n",
    "try:\n",
    "    dataset = pd.read_csv(\"dataset.csv\")\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error loading dataset:\", e)\n",
    "    \n",
    "# Removing unwanted column 'id'\n",
    "try:\n",
    "    dataset = dataset.drop('id', axis=1)\n",
    "    print(\"Column 'id' dropped successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Error dropping column 'id':\", e)\n",
    "\n",
    "# Check the dataset after dropping the column\n",
    "print(\"Updated dataset:\")\n",
    "\n",
    "x = dataset.iloc[ : , :-1].values\n",
    "y = dataset.iloc[:, -1:].values\n",
    "\n",
    "#spliting the dataset into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.25, random_state =0 )\n",
    "\n",
    "#----------------applying grid search to find best performing parameters \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = [{'n_estimators': [100, 700],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'criterion' :['gini', 'entropy']}]\n",
    "\n",
    "grid_search = GridSearchCV(RandomForestClassifier(),  parameters,cv =5, n_jobs= -1)\n",
    "grid_search.fit(x_train, y_train.ravel())\n",
    "#printing best parameters \n",
    "print(\"Best Accurancy =\" +str( grid_search.best_score_*100))\n",
    "print(\"best parameters =\" + str(grid_search.best_params_)) \n",
    "\n",
    "#fitting RandomForest regression with best params \n",
    "classifier = RandomForestClassifier(n_estimators = 100, criterion = \"gini\", max_features = 'log2',  random_state = 0)\n",
    "classifier.fit(x_train, y_train.ravel())\n",
    "\n",
    "#predicting the tests set result\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "#confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(cm)\n",
    "\n",
    "#pickle file joblib\n",
    "#joblib.dump(classifier, 'random_forest_model.pkl')\n",
    "jb.dump(classifier, 'random_forest_model.pkl')\n",
    "\n",
    "#-------------Features Importance random forest\n",
    "names = dataset.iloc[:,:-1].columns\n",
    "importances =classifier.feature_importances_\n",
    "sorted_importances = sorted(importances, reverse=True)\n",
    "indices = np.argsort(-importances)\n",
    "var_imp = pd.DataFrame(sorted_importances, names[indices], columns=['importance'])\n",
    "\n",
    "#-------------plotting variable importance\n",
    "plt.title(\"Variable Importances\")\n",
    "plt.barh(np.arange(len(names)), sorted_importances, height = 0.7)\n",
    "plt.yticks(np.arange(len(names)), names[indices], fontsize=7)\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79633608",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:62: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:63: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:64: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:65: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:66: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:68: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:151: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:411: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:413: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:414: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:415: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:416: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:417: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:418: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:62: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:63: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:64: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:65: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:66: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:67: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:68: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:151: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:411: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:413: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:414: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:415: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:416: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:417: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:418: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:62: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:63: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:64: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:65: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:66: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:67: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:68: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:151: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  '''\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:411: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:413: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:414: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:415: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:416: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:417: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
      "C:\\Users\\91904\\AppData\\Local\\Temp\\ipykernel_10588\\3792939453.py:418: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
      "D:\\Anaconda-new\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.3.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "D:\\Anaconda-new\\Lib\\site-packages\\sklearn\\base.py:376: InconsistentVersionWarning: Trying to unpickle estimator RandomForestClassifier from version 1.3.0 when using version 1.4.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter url:\n"
     ]
    }
   ],
   "source": [
    "import joblib as jb\n",
    "import ipaddress\n",
    "import requests\n",
    "import regex as re   \n",
    "from tldextract import extract\n",
    "import ssl\n",
    "import socket\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from dateutil.parser import parse as date_parse\n",
    "import datetime\n",
    "import whois\n",
    "\n",
    "# Calculates number of months\n",
    "def diff_month(d1, d2):\n",
    "    return (d1.year - d2.year) * 12 + d1.month - d2.month\n",
    "#load the pickle file\n",
    "#classifier = joblib.load('random_forest_model.pkl')\n",
    "classifier = jb.load('random_forest_model.pkl')\n",
    "\n",
    "#input url\n",
    "print(\"Enter url:\")\n",
    "url = input()\n",
    "\n",
    "if not re.match(r\"^https?\", url):\n",
    "    url = \"http://\" + url\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "except:\n",
    "    response = \"\"\n",
    "    soup = -999\n",
    "# Extracts domain from the given URL\n",
    "domain = re.findall(r\"://([^/]+)/?\", url)[0]\n",
    "# Requests all the information about the domain\n",
    "whois_response = requests.get(\"https://www.whois.com/whois/\"+domain)\n",
    "\n",
    "rank_checker_response = requests.post(\"https://www.checkpagerank.net/index.php\", {\n",
    "     \"name\": domain\n",
    "    })      \n",
    "\n",
    "# Extracts global rank of the website\n",
    "try:\n",
    "    global_rank = int(re.findall(r\"Global Rank: ([0-9]+)\", rank_checker_response.text)[0])\n",
    "except:\n",
    "    global_rank = -1      \n",
    "def url_having_ip(url):\n",
    "    try:\n",
    "        ipaddress.ip_address(url)\n",
    "        return 1\n",
    "    except:\n",
    "        return -1\n",
    "def url_length(url):\n",
    "    length=len(url)\n",
    "    if(length<54):\n",
    "        return -1\n",
    "    elif(54<=length<=75):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def url_short(url):\n",
    "    match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "    if match:\n",
    "        return -1\n",
    "    else:\n",
    "        return 1               \n",
    "def having_at_symbol(url):\n",
    "    symbol=re.findall(r'@',url)\n",
    "    if(len(symbol)==0):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1 \n",
    "def doubleSlash(url):\n",
    "    if re.findall(r\"[^https?:]//\",url):\n",
    "        return -1\n",
    "    else:\n",
    "        return 1\n",
    "def prefix_suffix(url):\n",
    "    subDomain, domain, suffix = extract(url)\n",
    "    if(domain.count('-')):\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "def sub_domain(url):\n",
    "    subDomain, domain, suffix = extract(url)\n",
    "    if(subDomain.count('.')==0):\n",
    "        return -1\n",
    "    elif(subDomain.count('.')==1):\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "def SSLfinal_State(url):\n",
    "    try:\n",
    "#check wheather contains https       \n",
    "        if(regex.search('^https',url)):\n",
    "            usehttps = 1\n",
    "        else:\n",
    "            usehttps = 0\n",
    "#getting the certificate issuer to later compare with trusted issuer \n",
    "        #getting host name\n",
    "        subDomain, domain, suffix = extract(url)\n",
    "        host_name = domain + \".\" + suffix\n",
    "        context = ssl.create_default_context()\n",
    "        sct = context.wrap_socket(socket.socket(), server_hostname = host_name)\n",
    "        sct.connect((host_name, 443))\n",
    "        certificate = sct.getpeercert()\n",
    "        issuer = dict(x[0] for x in certificate['issuer'])\n",
    "        certificate_Auth = str(issuer['commonName'])\n",
    "        certificate_Auth = certificate_Auth.split()\n",
    "        if(certificate_Auth[0] == \"Network\" or certificate_Auth == \"Deutsche\"):\n",
    "            certificate_Auth = certificate_Auth[0] + \" \" + certificate_Auth[1]\n",
    "        else:\n",
    "            certificate_Auth = certificate_Auth[0] \n",
    "        trusted_Auth = ['Comodo','Symantec','GoDaddy','GlobalSign','DigiCert','StartCom','Entrust','Verizon','Trustwave','Unizeto','Buypass','QuoVadis','Deutsche Telekom','Network Solutions','SwissSign','IdenTrust','Secom','TWCA','GeoTrust','Thawte','Doster','VeriSign']        \n",
    "#getting age of certificate\n",
    "        startingDate = str(certificate['notBefore'])\n",
    "        endingDate = str(certificate['notAfter'])\n",
    "        startingYear = int(startingDate.split()[3])\n",
    "        endingYear = int(endingDate.split()[3])\n",
    "        Age_of_certificate = endingYear-startingYear    \n",
    "#checking final conditions\n",
    "        if((usehttps==1) and (certificate_Auth in trusted_Auth) and (Age_of_certificate>=1) ):\n",
    "            return -1 #legitimate\n",
    "        elif((usehttps==1) and (certificate_Auth not in trusted_Auth)):\n",
    "            return 0 #suspicious\n",
    "        else:\n",
    "            return 1 #phishing     \n",
    "    except Exception as e:\n",
    "        return 1\n",
    "def domain_registration(url):\n",
    "    try:\n",
    "        w = whois.whois(url)\n",
    "        updated = w.updated_date\n",
    "        exp = w.expiration_date\n",
    "        length = (exp[0]-updated[0]).days\n",
    "        if(length<=365):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return 0\n",
    "def favicon(url):\n",
    "    #ongoing\n",
    "    return 1\n",
    "    '''\n",
    "    if soup == -999:\n",
    "        return -1\n",
    "    else:\n",
    "        for head in soup.find_all('head'):\n",
    "            for head.link in soup.find_all('link', href=True):\n",
    "                dots = [x.start(0) for x in re.finditer('\\.', head.link['href'])]\n",
    "                if url in head.link['href'] or len(dots) == 1 or domain in head.link['href']:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return -1\n",
    "     '''                   \n",
    "def port(url):\n",
    "    try:\n",
    "        port = domain.split(\":\")[1]\n",
    "        if port:\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return -1\n",
    "\n",
    "def https_token(url):\n",
    "    subDomain, domain, suffix = extract(url)\n",
    "    host =subDomain +'.' + domain + '.' + suffix \n",
    "    if(host.count('https')): #attacker can trick by putting https in domain part\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def request_url(url):\n",
    "    try:\n",
    "        subDomain, domain, suffix = extract(url)\n",
    "        websiteDomain = domain\n",
    "        \n",
    "        opener = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(opener, 'lxml')\n",
    "        imgs = soup.findAll('img', src=True)\n",
    "        total = len(imgs)\n",
    "        linked_to_same = 0\n",
    "        avg =0\n",
    "        for image in imgs:\n",
    "            subDomain, domain, suffix = extract(image['src'])\n",
    "            imageDomain = domain\n",
    "            if(websiteDomain==imageDomain or imageDomain==''):\n",
    "                linked_to_same = linked_to_same + 1\n",
    "        vids = soup.findAll('video', src=True)\n",
    "        total = total + len(vids)\n",
    "        \n",
    "        for video in vids:\n",
    "            subDomain, domain, suffix = extract(video['src'])\n",
    "            vidDomain = domain\n",
    "            if(websiteDomain==vidDomain or vidDomain==''):\n",
    "                linked_to_same = linked_to_same + 1\n",
    "                linked_outside = total-linked_to_same\n",
    "        if(total!=0):\n",
    "            avg = linked_outside/total\n",
    "        if(avg<0.22):\n",
    "            return -1\n",
    "        elif(0.22<=avg<=0.61):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 0\n",
    "def url_of_anchor(url):\n",
    "    try:\n",
    "        subDomain, domain, suffix = extract(url)\n",
    "        websiteDomain = domain\n",
    "        opener = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(opener, 'lxml')\n",
    "        anchors = soup.findAll('a', href=True)\n",
    "        total = len(anchors)\n",
    "        linked_to_same = 0\n",
    "        avg = 0\n",
    "        for anchor in anchors:\n",
    "            subDomain, domain, suffix = extract(anchor['href'])\n",
    "            anchorDomain = domain\n",
    "            if(websiteDomain==anchorDomain or anchorDomain==''):\n",
    "                linked_to_same = linked_to_same + 1\n",
    "        linked_outside = total-linked_to_same\n",
    "        if(total!=0):\n",
    "            avg = linked_outside/total     \n",
    "        if(avg<0.31):\n",
    "            return -1\n",
    "        elif(0.31<=avg<=0.67):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 0   \n",
    "def Links_in_tags(url):\n",
    "    try:\n",
    "        opener = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(opener, 'lxml')\n",
    "        no_of_meta =0\n",
    "        no_of_link =0\n",
    "        no_of_script =0\n",
    "        anchors=0\n",
    "        avg =0\n",
    "        for meta in soup.find_all('meta'):\n",
    "            no_of_meta = no_of_meta+1\n",
    "        for link in soup.find_all('link'):\n",
    "            no_of_link = no_of_link +1\n",
    "        for script in soup.find_all('script'):\n",
    "            no_of_script = no_of_script+1\n",
    "        for anchor in soup.find_all('a'):\n",
    "            anchors = anchors+1\n",
    "        total = no_of_meta + no_of_link + no_of_script+anchors\n",
    "        tags = no_of_meta + no_of_link + no_of_script\n",
    "        if(total!=0):\n",
    "            avg = tags/total\n",
    "\n",
    "        if(avg<0.25):\n",
    "            return -1\n",
    "        elif(0.25<=avg<=0.81):\n",
    "            return 0\n",
    "        else:\n",
    "            return 1        \n",
    "    except:        \n",
    "        return 0\n",
    "def sfh(url): \n",
    "    ''''\n",
    "    try:\n",
    "        for form in soup.find_all('form', action= True):\n",
    "           if form['action'] ==\"\" or form['action'] == \"about:blank\" :\n",
    "              return -1\n",
    "              break\n",
    "           elif url not in form['action'] and domain not in form['action']:\n",
    "               return 0\n",
    "               break\n",
    "           else:\n",
    "               return 1\n",
    "               break\n",
    "    except:\n",
    "        '''\n",
    "    return 0         \n",
    "def email_submit(url):\n",
    "    try:\n",
    "        opener = urllib.request.urlopen(url).read()\n",
    "        soup = BeautifulSoup(opener, 'lxml')\n",
    "        if(soup.find('mailto:')):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1 \n",
    "    except:\n",
    "        return 0\n",
    "    return 0\n",
    "def abnormal_url(url):\n",
    "    try:\n",
    "        if response.text == \"\":\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return 0\n",
    "def redirect(url):\n",
    "    try: \n",
    "        if len(response.history) <= 1:\n",
    "            return -1\n",
    "        elif len(response.history) <= 4:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 0\n",
    "def on_mouseover(url):\n",
    "    try:\n",
    "        if re.findall(\"<script>.+onmouseover.+</script>\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return 0\n",
    "def rightClick(url):\n",
    "    try:\n",
    "        if re.findall(r\"event.button ?== ?2\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return 0\n",
    "def popup(url):\n",
    "    try:\n",
    "        if re.findall(r\"alert\\(\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return 0\n",
    "def iframe(url):\n",
    "    try:\n",
    "        if re.findall(r\"[<iframe>|<frameBorder>]\", response.text):\n",
    "            return 1\n",
    "        else:\n",
    "            return -1\n",
    "    except:\n",
    "        return 0\n",
    "def age_of_domain(url):\n",
    "    try:\n",
    "        if response == \"\":\n",
    "            data_set.append(-1)\n",
    "        else:\n",
    "            try:\n",
    "                registration_date = re.findall(r'Registration Date:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
    "                if diff_month(date.today(), date_parse(registration_date)) >= 6:\n",
    "                    return -1\n",
    "                else:\n",
    "                    return 1\n",
    "            except:\n",
    "                    return 1\n",
    "    except:\n",
    "        return 0        \n",
    "def dns(url):\n",
    "    dns = 1\n",
    "    try:\n",
    "        registration_date = re.findall(r'Registration Date:</div><div class=\"df-value\">([^<]+)</div>', whois_response.text)[0]\n",
    "        d = whois.whois(domain)\n",
    "    except:\n",
    "        dns=-1\n",
    "    if dns == -1:\n",
    "        return -1\n",
    "    else:\n",
    "        if registration_length / 365 <= 1:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "def web_traffic(url):\n",
    "    try:\n",
    "        if global_rank > 0 and global_rank < 100000:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 1\n",
    "def page_rank(url):\n",
    "    try:\n",
    "        if global_rank > 0 and global_rank < 100000:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 1\n",
    "def google_index(url):\n",
    "    try:\n",
    "        if global_rank > 0 and global_rank < 100000:\n",
    "            return -1\n",
    "        else:\n",
    "            return 1\n",
    "    except:\n",
    "        return 1\n",
    "def links_pointing(url):\n",
    "    number_of_links = len(re.findall(r\"<a href=\", response.text))\n",
    "    if number_of_links == 0:\n",
    "        return 1\n",
    "    elif number_of_links <= 2:\n",
    "        return 0\n",
    "    else:\n",
    "        return -1\n",
    "def statistical(url):\n",
    "   url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
    "   ip_address=socket.gethostbyname(domain)\n",
    "   ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|'\n",
    "                           '107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|'\n",
    "                           '118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|'\n",
    "                           '216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|'\n",
    "                           '34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|'\n",
    "                           '216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)\n",
    "   if url_match:\n",
    "        return -1\n",
    "   elif ip_match:\n",
    "        return -1\n",
    "   else:\n",
    "        return 1\n",
    "def main(url):\n",
    "    check = [[url_having_ip(url),url_length(url),url_short(url),having_at_symbol(url),\n",
    "             doubleSlash(url),prefix_suffix(url),sub_domain(url),SSLfinal_State(url),\n",
    "              domain_registration(url),favicon(url),port(url),https_token(url),request_url(url),\n",
    "              url_of_anchor(url),Links_in_tags(url),sfh(url),email_submit(url),abnormal_url(url),\n",
    "              redirect(url),on_mouseover(url),rightClick(url),popup(url),iframe(url),\n",
    "              age_of_domain(url),dns(url),web_traffic(url),page_rank(url),google_index(url),\n",
    "              links_pointing(url),statistical(url)]]\n",
    "    print(check)\n",
    "    return check\n",
    "    print(imgs)\n",
    "#checking and predicting\n",
    "checkprediction=main(url)\n",
    "prediction = classifier.predict(checkprediction)\n",
    "print(prediction)\n",
    "if prediction==1:\n",
    "    print('Phishing website')\n",
    "elif prediction==-1:\n",
    "    print('Legitimate website')\n",
    "else:\n",
    "    print('No Data Found')\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3ad8321",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1564042302.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[3], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    https://gf.ebayn.xyz/\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "https://gf.ebayn.xyz/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ca5e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.instagram.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
